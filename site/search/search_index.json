{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Welcome to my site.</p> <p>You will find various articles here on technical subjects which interest me. Most of these articles were written by AI agents in the course of conversations had with me. The conversations are grouped together as project folders in the left sidebar. </p> <p>I use Obsidian to organise and view that material locally and git to manage version control and distribution. </p> <p>Content is created by AI in Markdown, converted into html by Material for MKDocs and published to GitHub Pages. </p> <p>Diagrams are generated using Mermaid embedded markdown. </p> <pre><code>flowchart TD\n  A[Markdown] --&gt; B[MkDocs build]\n  B --&gt; C[GitHub Pages]</code></pre>"},{"location":"LLM/attention-and-weights-relationship/","title":"The Relationship Between Attention and Weights","text":""},{"location":"LLM/attention-and-weights-relationship/#the-short-answer","title":"The Short Answer","text":"<p>Weights are the stored, learned parameters. Attention is a computation that uses those weights to determine how tokens relate to each other at runtime.</p> <pre><code>Weights = Stored (learned during training)\nAttention Scores = Computed (calculated fresh for every input)\n</code></pre>"},{"location":"LLM/attention-and-weights-relationship/#the-two-types-of-weights-people-confuse","title":"The Two Types of \"Weights\" People Confuse","text":"Term What It Is Stored in GGUF? Model Weights Learned parameters (W_q, W_k, W_v matrices) \u2705 Yes Attention Weights/Scores Token-to-token relevance scores \u274c No (computed at runtime) <p>This naming collision causes confusion. When someone says \"attention weights,\" they usually mean the attention scores \u2014 which are not stored.</p>"},{"location":"LLM/attention-and-weights-relationship/#how-attention-works","title":"How Attention Works","text":"<pre><code>flowchart TB\n    subgraph STORED[\"STORED IN GGUF (Learned Parameters)\"]\n        WQ[\"W_q&lt;br/&gt;Query Weight Matrix&lt;br/&gt;[4096 \u00d7 4096]\"]\n        WK[\"W_k&lt;br/&gt;Key Weight Matrix&lt;br/&gt;[4096 \u00d7 4096]\"]\n        WV[\"W_v&lt;br/&gt;Value Weight Matrix&lt;br/&gt;[4096 \u00d7 4096]\"]\n        WO[\"W_o&lt;br/&gt;Output Weight Matrix&lt;br/&gt;[4096 \u00d7 4096]\"]\n    end\n\n    subgraph COMPUTED[\"COMPUTED AT RUNTIME\"]\n        Q[\"Queries (Q)\"]\n        K[\"Keys (K)\"]\n        V[\"Values (V)\"]\n        SCORES[\"Attention Scores&lt;br/&gt;(which tokens attend to which)\"]\n        OUT[\"Attention Output\"]\n    end\n\n    WQ --&gt; Q\n    WK --&gt; K\n    WV --&gt; V\n    Q &amp; K --&gt; SCORES\n    SCORES &amp; V --&gt; OUT\n    WO --&gt; OUT\n\n    style STORED fill:#4a90d9,stroke:#2c5282,color:#fff\n    style COMPUTED fill:#ed8936,stroke:#c05621,color:#fff</code></pre>"},{"location":"LLM/attention-and-weights-relationship/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":""},{"location":"LLM/attention-and-weights-relationship/#step-1-project-input-using-stored-weights","title":"Step 1: Project Input Using Stored Weights","text":"<p>For each token's embedding vector <code>x</code>:</p> <pre><code>Q = x \u00d7 W_q    (Query: \"What am I looking for?\")\nK = x \u00d7 W_k    (Key: \"What do I contain?\")\nV = x \u00d7 W_v    (Value: \"What information do I provide?\")\n</code></pre> <p>The weight matrices <code>W_q</code>, <code>W_k</code>, <code>W_v</code> are stored in the GGUF file. They were learned during training.</p>"},{"location":"LLM/attention-and-weights-relationship/#step-2-compute-attention-scores-runtime","title":"Step 2: Compute Attention Scores (Runtime)","text":"<pre><code>Attention_Scores = softmax( (Q \u00d7 K^T) / \u221ad )\n</code></pre> <p>This produces a matrix showing how much each token should \"pay attention\" to every other token.</p> <pre><code>flowchart LR\n    subgraph SCORE_MATRIX[\"ATTENTION SCORE MATRIX (Computed)\"]\n        direction TB\n        M[\"         The  cat  sat  on   mat\n        The  [1.0, 0.0, 0.0, 0.0, 0.0]\n        cat  [0.3, 0.5, 0.1, 0.0, 0.1]\n        sat  [0.2, 0.4, 0.2, 0.1, 0.1]\n        on   [0.1, 0.2, 0.3, 0.2, 0.2]\n        mat  [0.1, 0.3, 0.1, 0.2, 0.3]\"]\n    end\n\n    style SCORE_MATRIX fill:#f6e05e,stroke:#d69e2e,color:#333</code></pre> <p>This matrix is NOT stored \u2014 it's computed fresh for every input sequence.</p>"},{"location":"LLM/attention-and-weights-relationship/#step-3-apply-scores-to-values","title":"Step 3: Apply Scores to Values","text":"<pre><code>Output = Attention_Scores \u00d7 V\n</code></pre> <p>Each token's output is a weighted combination of all Value vectors, where the weights come from the attention scores.</p>"},{"location":"LLM/attention-and-weights-relationship/#step-4-project-through-output-weight","title":"Step 4: Project Through Output Weight","text":"<pre><code>Final_Output = Output \u00d7 W_o\n</code></pre> <p>The output weight matrix <code>W_o</code> is stored in the GGUF file.</p>"},{"location":"LLM/attention-and-weights-relationship/#visual-stored-vs-computed","title":"Visual: Stored vs Computed","text":"<pre><code>flowchart TB\n    subgraph GGUF[\"GGUF FILE (Stored)\"]\n        direction LR\n        W1[\"W_q\"]\n        W2[\"W_k\"]\n        W3[\"W_v\"]\n        W4[\"W_o\"]\n    end\n\n    subgraph RUNTIME[\"INFERENCE (Computed)\"]\n        direction TB\n        INPUT[\"Input: 'The cat sat'\"]\n        QKV[\"Q, K, V vectors\"]\n        ATT[\"Attention Scores&lt;br/&gt;(token relationships)\"]\n        OUTPUT[\"Output vectors\"]\n    end\n\n    GGUF --&gt;|\"used to compute\"| RUNTIME\n    INPUT --&gt; QKV --&gt; ATT --&gt; OUTPUT\n\n    style GGUF fill:#9f7aea,stroke:#6b46c1,color:#fff\n    style RUNTIME fill:#48bb78,stroke:#276749,color:#fff</code></pre>"},{"location":"LLM/attention-and-weights-relationship/#why-this-design","title":"Why This Design?","text":""},{"location":"LLM/attention-and-weights-relationship/#the-power-of-learned-projections","title":"The Power of Learned Projections","text":"<p>The weight matrices learn how to create good queries, keys, and values during training:</p> Matrix What It Learns <code>W_q</code> How to formulate \"questions\" about what information is needed <code>W_k</code> How to create \"labels\" describing what each token contains <code>W_v</code> How to package the actual information to be retrieved <code>W_o</code> How to combine multi-head outputs into a useful representation"},{"location":"LLM/attention-and-weights-relationship/#dynamic-relationships","title":"Dynamic Relationships","text":"<p>Because attention scores are computed at runtime:</p> <ul> <li>The same model can handle any input text</li> <li>Token relationships are context-dependent</li> <li>\"Bank\" attends to \"river\" differently than to \"money\"</li> </ul>"},{"location":"LLM/attention-and-weights-relationship/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Real models use multiple attention \"heads\" in parallel:</p> <pre><code>flowchart TB\n    subgraph MULTIHEAD[\"MULTI-HEAD ATTENTION\"]\n        INPUT[\"Input\"]\n\n        subgraph HEADS[\"Parallel Attention Heads\"]\n            H1[\"Head 1&lt;br/&gt;W_q1, W_k1, W_v1\"]\n            H2[\"Head 2&lt;br/&gt;W_q2, W_k2, W_v2\"]\n            H3[\"Head 3&lt;br/&gt;W_q3, W_k3, W_v3\"]\n            HN[\"Head N&lt;br/&gt;...\"]\n        end\n\n        CONCAT[\"Concatenate\"]\n        WO[\"W_o (stored)\"]\n        OUTPUT[\"Output\"]\n    end\n\n    INPUT --&gt; H1 &amp; H2 &amp; H3 &amp; HN\n    H1 &amp; H2 &amp; H3 &amp; HN --&gt; CONCAT --&gt; WO --&gt; OUTPUT\n\n    style HEADS fill:#4a90d9,stroke:#2c5282,color:#fff\n    style WO fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre> <p>Each head has its own stored weight matrices but computes its own attention scores at runtime. Different heads can learn to focus on different types of relationships:</p> <ul> <li>Head 1: Syntactic relationships (subject-verb)</li> <li>Head 2: Positional relationships (nearby words)</li> <li>Head 3: Semantic relationships (synonyms, concepts)</li> </ul>"},{"location":"LLM/attention-and-weights-relationship/#concrete-example","title":"Concrete Example","text":"<p>Given the input: \"The cat sat on the mat\"</p>"},{"location":"LLM/attention-and-weights-relationship/#stored-in-gguf","title":"Stored (in GGUF):","text":"<pre><code>blk.0.attn_q.weight  = [4096 \u00d7 4096 matrix of floats]\nblk.0.attn_k.weight  = [4096 \u00d7 4096 matrix of floats]\nblk.0.attn_v.weight  = [4096 \u00d7 4096 matrix of floats]\nblk.0.attn_output.weight = [4096 \u00d7 4096 matrix of floats]\n</code></pre>"},{"location":"LLM/attention-and-weights-relationship/#computed-at-runtime","title":"Computed (at runtime):","text":"<pre><code>Q for \"cat\" = embedding(\"cat\") \u00d7 W_q = [0.12, -0.45, 0.89, ...]\nK for \"sat\" = embedding(\"sat\") \u00d7 W_k = [0.34, 0.21, -0.56, ...]\n\nAttention score (\"cat\" \u2192 \"sat\") = softmax(Q_cat \u00b7 K_sat / \u221ad) = 0.42\n</code></pre> <p>This score 0.42 means \"cat\" pays 42% of its attention to \"sat\" in this context.</p>"},{"location":"LLM/attention-and-weights-relationship/#the-attention-formula","title":"The Attention Formula","text":"<pre><code>Attention(Q, K, V) = softmax(Q \u00d7 K^T / \u221ad_k) \u00d7 V\n</code></pre> Component Source Stored? Q Input \u00d7 W_q W_q stored, Q computed K Input \u00d7 W_k W_k stored, K computed V Input \u00d7 W_v W_v stored, V computed \u221ad_k Constant (embedding dimension) N/A softmax(...) Attention scores Computed Final result Weighted sum of values Computed"},{"location":"LLM/attention-and-weights-relationship/#summary-table","title":"Summary Table","text":"Concept Stored in GGUF? Purpose W_q, W_k, W_v, W_o \u2705 Yes Transform inputs into Q, K, V Attention scores \u274c No Determine token-to-token relevance Q, K, V vectors \u274c No Intermediate representations Final attention output \u274c No Contextualized token representations"},{"location":"LLM/attention-and-weights-relationship/#key-insight","title":"Key Insight","text":"<pre><code>flowchart LR\n    subgraph RELATIONSHIP[\"THE RELATIONSHIP\"]\n        WEIGHTS[\"Stored Weights&lt;br/&gt;(learned parameters)\"]\n        ATTENTION[\"Attention Mechanism&lt;br/&gt;(runtime computation)\"]\n        MEANING[\"Contextual Meaning&lt;br/&gt;(output)\"]\n\n        WEIGHTS --&gt;|\"enable\"| ATTENTION --&gt;|\"produces\"| MEANING\n    end\n\n    style WEIGHTS fill:#9f7aea,stroke:#6b46c1,color:#fff\n    style ATTENTION fill:#ed8936,stroke:#c05621,color:#fff\n    style MEANING fill:#48bb78,stroke:#276749,color:#fff</code></pre> <p>Weights are the \"recipe\" \u2014 they define the transformations. Attention is the \"cooking\" \u2014 it applies those transformations to produce context-aware understanding.</p> <p>The weights learn how to compute good attention. The attention scores themselves emerge from the input.</p> <p>Guide explaining the relationship between attention and weights in transformer models</p>"},{"location":"LLM/gguf-file-structure-guide/","title":"GGUF File Format: Complete Structural Guide","text":""},{"location":"LLM/gguf-file-structure-guide/#overview","title":"Overview","text":"<p>GGUF (GPT-Generated Unified Format) is a binary file format designed for storing large language models. It is the successor to the older GGML format and is primarily used by llama.cpp and compatible inference engines. GGUF is optimized for fast loading, memory-mapped access, and single-file distribution of quantized models.</p>"},{"location":"LLM/gguf-file-structure-guide/#high-level-file-structure","title":"High-Level File Structure","text":"<p>A GGUF file is organized into four major logical regions, read sequentially:</p> <pre><code>flowchart TB\n    subgraph GGUF[\"GGUF FILE STRUCTURE\"]\n        direction TB\n        A[\"\ud83d\udccb HEADER&lt;br/&gt;Magic + Version + Counts\"]\n        B[\"\ud83c\udff7\ufe0f METADATA KV PAIRS&lt;br/&gt;Model Configuration\"]\n        C[\"\ud83d\udcd0 TENSOR INFOS&lt;br/&gt;Tensor Descriptors\"]\n        D[\"\ud83d\udcbe TENSOR DATA&lt;br/&gt;Quantized Weights\"]\n    end\n\n    A --&gt; B --&gt; C --&gt; D\n\n    style A fill:#4a90d9,stroke:#2c5282,color:#fff\n    style B fill:#48bb78,stroke:#276749,color:#fff\n    style C fill:#ed8936,stroke:#c05621,color:#fff\n    style D fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#section-1-header","title":"Section 1: Header","text":"<p>The header is the first 24 bytes of every GGUF file and contains essential identification and counting information.</p>"},{"location":"LLM/gguf-file-structure-guide/#header-layout","title":"Header Layout","text":"<pre><code>flowchart LR\n    subgraph HEADER[\"HEADER (24 bytes)\"]\n        direction LR\n        M[\"MAGIC&lt;br/&gt;4 bytes&lt;br/&gt;'GGUF'\"]\n        V[\"VERSION&lt;br/&gt;4 bytes&lt;br/&gt;uint32\"]\n        T[\"TENSOR_COUNT&lt;br/&gt;8 bytes&lt;br/&gt;uint64\"]\n        K[\"METADATA_KV_COUNT&lt;br/&gt;8 bytes&lt;br/&gt;uint64\"]\n    end\n\n    M --&gt; V --&gt; T --&gt; K\n\n    style M fill:#e53e3e,stroke:#c53030,color:#fff\n    style V fill:#dd6b20,stroke:#c05621,color:#fff\n    style T fill:#d69e2e,stroke:#b7791f,color:#fff\n    style K fill:#38a169,stroke:#276749,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#header-fields-explained","title":"Header Fields Explained","text":"Offset Size Field Type Description 0x00 4 Magic char[4] ASCII string <code>GGUF</code> (0x47475546) - identifies file format 0x04 4 Version uint32_t Format version (currently 3) 0x08 8 Tensor Count uint64_t Number of tensors stored in the file 0x10 8 Metadata KV Count uint64_t Number of metadata key-value pairs"},{"location":"LLM/gguf-file-structure-guide/#version-history","title":"Version History","text":"<pre><code>timeline\n    title GGUF Version Evolution\n    section v1\n        Initial Release : Basic structure\n    section v2  \n        Alignment : Added padding for memory mapping\n    section v3\n        Current : Big-endian support, improved compatibility</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#section-2-metadata-key-value-pairs","title":"Section 2: Metadata Key-Value Pairs","text":"<p>Immediately following the header, metadata is stored as a sequence of key-value pairs. This section contains all model configuration, architecture details, tokenizer data, and custom attributes.</p>"},{"location":"LLM/gguf-file-structure-guide/#metadata-structure-overview","title":"Metadata Structure Overview","text":"<pre><code>flowchart TB\n    subgraph META[\"METADATA SECTION\"]\n        direction TB\n        KV1[\"Key-Value Pair 1\"]\n        KV2[\"Key-Value Pair 2\"]\n        KV3[\"Key-Value Pair 3\"]\n        KVN[\"Key-Value Pair N...\"]\n    end\n\n    subgraph KVPAIR[\"SINGLE KV PAIR STRUCTURE\"]\n        direction LR\n        KL[\"Key Length&lt;br/&gt;uint64\"]\n        KS[\"Key String&lt;br/&gt;UTF-8\"]\n        VT[\"Value Type&lt;br/&gt;uint32\"]\n        VD[\"Value Data&lt;br/&gt;variable\"]\n    end\n\n    KV1 --&gt; KV2 --&gt; KV3 --&gt; KVN\n\n    style KL fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style KS fill:#48bb78,stroke:#2f855a,color:#fff\n    style VT fill:#ed8936,stroke:#c05621,color:#fff\n    style VD fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#value-types","title":"Value Types","text":"<p>GGUF supports multiple data types for metadata values:</p> <pre><code>flowchart TB\n    subgraph TYPES[\"GGUF VALUE TYPES\"]\n        direction TB\n\n        subgraph SCALAR[\"SCALAR TYPES\"]\n            T0[\"0: UINT8\"]\n            T1[\"1: INT8\"]\n            T2[\"2: UINT16\"]\n            T3[\"3: INT16\"]\n            T4[\"4: UINT32\"]\n            T5[\"5: INT32\"]\n            T6[\"6: FLOAT32\"]\n            T7[\"7: BOOL\"]\n            T10[\"10: UINT64\"]\n            T11[\"11: INT64\"]\n            T12[\"12: FLOAT64\"]\n        end\n\n        subgraph COMPLEX[\"COMPLEX TYPES\"]\n            T8[\"8: STRING&lt;br/&gt;length + UTF-8 data\"]\n            T9[\"9: ARRAY&lt;br/&gt;type + count + elements\"]\n        end\n    end\n\n    style T8 fill:#48bb78,stroke:#2f855a,color:#fff\n    style T9 fill:#ed8936,stroke:#c05621,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#type-id-reference-table","title":"Type ID Reference Table","text":"Type ID Name Size Description 0 UINT8 1 byte Unsigned 8-bit integer 1 INT8 1 byte Signed 8-bit integer 2 UINT16 2 bytes Unsigned 16-bit integer 3 INT16 2 bytes Signed 16-bit integer 4 UINT32 4 bytes Unsigned 32-bit integer 5 INT32 4 bytes Signed 32-bit integer 6 FLOAT32 4 bytes 32-bit floating point 7 BOOL 1 byte Boolean (0 or 1) 8 STRING variable Length-prefixed UTF-8 string 9 ARRAY variable Homogeneous typed array 10 UINT64 8 bytes Unsigned 64-bit integer 11 INT64 8 bytes Signed 64-bit integer 12 FLOAT64 8 bytes 64-bit floating point"},{"location":"LLM/gguf-file-structure-guide/#common-metadata-keys","title":"Common Metadata Keys","text":"<pre><code>mindmap\n  root((Metadata&lt;br/&gt;Keys))\n    General\n      general.architecture\n      general.name\n      general.author\n      general.license\n      general.quantization_version\n    Architecture\n      llama.context_length\n      llama.embedding_length\n      llama.block_count\n      llama.attention.head_count\n      llama.attention.head_count_kv\n      llama.rope.freq_base\n    Tokenizer\n      tokenizer.ggml.model\n      tokenizer.ggml.tokens\n      tokenizer.ggml.scores\n      tokenizer.ggml.token_type\n      tokenizer.ggml.bos_token_id\n      tokenizer.ggml.eos_token_id</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#string-encoding-detail","title":"String Encoding Detail","text":"<pre><code>flowchart LR\n    subgraph STRING[\"STRING VALUE ENCODING\"]\n        direction LR\n        SL[\"String Length&lt;br/&gt;uint64&lt;br/&gt;8 bytes\"]\n        SD[\"String Data&lt;br/&gt;UTF-8 bytes&lt;br/&gt;N bytes\"]\n    end\n\n    SL --&gt; SD\n\n    style SL fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style SD fill:#48bb78,stroke:#2f855a,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#array-encoding-detail","title":"Array Encoding Detail","text":"<pre><code>flowchart LR\n    subgraph ARRAY[\"ARRAY VALUE ENCODING\"]\n        direction LR\n        AT[\"Element Type&lt;br/&gt;uint32&lt;br/&gt;4 bytes\"]\n        AC[\"Array Count&lt;br/&gt;uint64&lt;br/&gt;8 bytes\"]\n        AE[\"Elements&lt;br/&gt;type \u00d7 count&lt;br/&gt;variable\"]\n    end\n\n    AT --&gt; AC --&gt; AE\n\n    style AT fill:#ed8936,stroke:#c05621,color:#fff\n    style AC fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style AE fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#section-3-tensor-information","title":"Section 3: Tensor Information","text":"<p>After metadata, the file contains descriptors for each tensor. These descriptors define tensor names, shapes, data types, and offsets into the data section.</p>"},{"location":"LLM/gguf-file-structure-guide/#tensor-info-structure","title":"Tensor Info Structure","text":"<pre><code>flowchart TB\n    subgraph TENSORS[\"TENSOR INFO SECTION\"]\n        direction TB\n        T1[\"Tensor Info 1\"]\n        T2[\"Tensor Info 2\"]\n        T3[\"Tensor Info 3\"]\n        TN[\"Tensor Info N...\"]\n    end\n\n    subgraph TINFO[\"SINGLE TENSOR INFO\"]\n        direction LR\n        NL[\"Name Length&lt;br/&gt;uint64\"]\n        NS[\"Name String&lt;br/&gt;UTF-8\"]\n        ND[\"N Dimensions&lt;br/&gt;uint32\"]\n        DM[\"Dimensions&lt;br/&gt;uint64 \u00d7 N\"]\n        TT[\"Tensor Type&lt;br/&gt;uint32\"]\n        OF[\"Data Offset&lt;br/&gt;uint64\"]\n    end\n\n    T1 --&gt; T2 --&gt; T3 --&gt; TN\n\n    style NL fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style NS fill:#48bb78,stroke:#2f855a,color:#fff\n    style ND fill:#ed8936,stroke:#c05621,color:#fff\n    style DM fill:#f6e05e,stroke:#d69e2e,color:#333\n    style TT fill:#fc8181,stroke:#c53030,color:#fff\n    style OF fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#tensor-info-fields","title":"Tensor Info Fields","text":"Field Type Description Name Length uint64_t Length of tensor name in bytes Name char[] UTF-8 encoded tensor name N Dimensions uint32_t Number of dimensions (1-4 typically) Dimensions uint64_t[] Size of each dimension Type uint32_t Quantization/data type ID Offset uint64_t Byte offset from start of tensor data section"},{"location":"LLM/gguf-file-structure-guide/#tensorquantization-types","title":"Tensor/Quantization Types","text":"<pre><code>flowchart TB\n    subgraph QTYPES[\"TENSOR DATA TYPES\"]\n        direction LR\n\n        subgraph FULL[\"FULL PRECISION\"]\n            F32[\"F32&lt;br/&gt;32-bit float\"]\n            F16[\"F16&lt;br/&gt;16-bit float\"]\n            BF16[\"BF16&lt;br/&gt;bfloat16\"]\n        end\n\n        subgraph LEGACY[\"LEGACY QUANT\"]\n            Q4_0[\"Q4_0&lt;br/&gt;4-bit (32 block)\"]\n            Q4_1[\"Q4_1&lt;br/&gt;4-bit + min\"]\n            Q5_0[\"Q5_0&lt;br/&gt;5-bit\"]\n            Q5_1[\"Q5_1&lt;br/&gt;5-bit + min\"]\n            Q8_0[\"Q8_0&lt;br/&gt;8-bit\"]\n        end\n\n        subgraph KQUANT[\"K-QUANTS\"]\n            Q2_K[\"Q2_K&lt;br/&gt;2-bit k-quant\"]\n            Q3_K[\"Q3_K&lt;br/&gt;3-bit k-quant\"]\n            Q4_K[\"Q4_K&lt;br/&gt;4-bit k-quant\"]\n            Q5_K[\"Q5_K&lt;br/&gt;5-bit k-quant\"]\n            Q6_K[\"Q6_K&lt;br/&gt;6-bit k-quant\"]\n        end\n\n        subgraph IQUANT[\"I-QUANTS\"]\n            IQ2[\"IQ2_*&lt;br/&gt;2-bit importance\"]\n            IQ3[\"IQ3_*&lt;br/&gt;3-bit importance\"]\n            IQ4[\"IQ4_*&lt;br/&gt;4-bit importance\"]\n        end\n    end\n\n    style F32 fill:#48bb78,stroke:#2f855a,color:#fff\n    style F16 fill:#4299e1,stroke:#2b6cb0,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#quantization-type-ids","title":"Quantization Type IDs","text":"ID Type Bits/Weight Block Size Description 0 F32 32 1 Full 32-bit float 1 F16 16 1 Half precision float 2 Q4_0 4 32 4-bit quantization 3 Q4_1 4.5 32 4-bit with min value 6 Q5_0 5 32 5-bit quantization 7 Q5_1 5.5 32 5-bit with min value 8 Q8_0 8 32 8-bit quantization 10 Q2_K 2.5 256 K-quant 2-bit 11 Q3_K 3.4 256 K-quant 3-bit 12 Q4_K 4.5 256 K-quant 4-bit 13 Q5_K 5.5 256 K-quant 5-bit 14 Q6_K 6.5 256 K-quant 6-bit 30 BF16 16 1 Brain float 16"},{"location":"LLM/gguf-file-structure-guide/#section-4-tensor-data","title":"Section 4: Tensor Data","text":"<p>The final and largest section contains the actual tensor weights. Data is aligned for efficient memory-mapped access.</p>"},{"location":"LLM/gguf-file-structure-guide/#alignment-and-padding","title":"Alignment and Padding","text":"<pre><code>flowchart TB\n    subgraph ALIGN[\"DATA ALIGNMENT\"]\n        direction TB\n\n        END_META[\"End of Tensor Info Section\"]\n        PAD[\"ALIGNMENT PADDING&lt;br/&gt;Padded to 32-byte boundary\"]\n        DATA[\"TENSOR DATA START&lt;br/&gt;Aligned address\"]\n\n        END_META --&gt; PAD --&gt; DATA\n    end\n\n    subgraph LAYOUT[\"TENSOR DATA LAYOUT\"]\n        direction LR\n        T1D[\"Tensor 1 Data\"]\n        T2D[\"Tensor 2 Data\"]\n        T3D[\"Tensor 3 Data\"]\n        TND[\"Tensor N Data...\"]\n    end\n\n    DATA --&gt; LAYOUT\n    T1D --&gt; T2D --&gt; T3D --&gt; TND\n\n    style PAD fill:#fc8181,stroke:#c53030,color:#fff\n    style DATA fill:#48bb78,stroke:#2f855a,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#data-section-properties","title":"Data Section Properties","text":"<ul> <li>Alignment: Data starts at a 32-byte aligned offset (configurable, default 32)</li> <li>Tensor Order: Tensors are stored in the order their info appears</li> <li>No Padding Between Tensors: Tensors are packed contiguously (type-specific alignment may apply)</li> <li>Offsets: Each tensor's offset is relative to the start of the data section</li> </ul>"},{"location":"LLM/gguf-file-structure-guide/#complete-file-layout","title":"Complete File Layout","text":"<pre><code>flowchart TB\n    subgraph FILE[\"COMPLETE GGUF FILE BYTE LAYOUT\"]\n        direction TB\n\n        subgraph H[\"BYTES 0-23: HEADER\"]\n            H1[\"0x00-0x03: Magic 'GGUF'\"]\n            H2[\"0x04-0x07: Version\"]\n            H3[\"0x08-0x0F: Tensor Count\"]\n            H4[\"0x10-0x17: KV Count\"]\n        end\n\n        subgraph M[\"VARIABLE: METADATA\"]\n            M1[\"KV Pair 1\"]\n            M2[\"KV Pair 2\"]\n            M3[\"...\"]\n            MN[\"KV Pair N\"]\n        end\n\n        subgraph T[\"VARIABLE: TENSOR INFOS\"]\n            T1[\"Tensor Info 1\"]\n            T2[\"Tensor Info 2\"]\n            T3[\"...\"]\n            TN[\"Tensor Info N\"]\n        end\n\n        subgraph P[\"PADDING\"]\n            P1[\"Alignment Padding&lt;br/&gt;to 32-byte boundary\"]\n        end\n\n        subgraph D[\"BULK: TENSOR DATA\"]\n            D1[\"Tensor 1 Weights\"]\n            D2[\"Tensor 2 Weights\"]\n            D3[\"...\"]\n            DN[\"Tensor N Weights\"]\n        end\n    end\n\n    H --&gt; M --&gt; T --&gt; P --&gt; D\n\n    style H fill:#4a90d9,stroke:#2c5282,color:#fff\n    style M fill:#48bb78,stroke:#276749,color:#fff\n    style T fill:#ed8936,stroke:#c05621,color:#fff\n    style P fill:#a0aec0,stroke:#718096,color:#fff\n    style D fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#memory-mapping","title":"Memory Mapping","text":"<p>GGUF is designed for efficient memory-mapped loading:</p> <pre><code>flowchart LR\n    subgraph DISK[\"DISK FILE\"]\n        DF[\"GGUF File\"]\n    end\n\n    subgraph MMAP[\"MEMORY MAP\"]\n        VM[\"Virtual Memory&lt;br/&gt;Pages\"]\n    end\n\n    subgraph RAM[\"PHYSICAL RAM\"]\n        PM[\"Loaded Pages&lt;br/&gt;On Demand\"]\n    end\n\n    DF --&gt;|\"mmap()\"| VM\n    VM --&gt;|\"Page Fault\"| PM\n\n    style DF fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style VM fill:#ed8936,stroke:#c05621,color:#fff\n    style PM fill:#48bb78,stroke:#2f855a,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#benefits-of-memory-mapping","title":"Benefits of Memory Mapping","text":"<ol> <li>Fast Load Times: Only metadata is read initially</li> <li>On-Demand Loading: Tensor pages loaded only when accessed</li> <li>Shared Memory: Multiple processes can share the same mapped file</li> <li>Low Memory Overhead: No need to copy data from disk buffer</li> </ol>"},{"location":"LLM/gguf-file-structure-guide/#typical-model-architecture-in-gguf","title":"Typical Model Architecture in GGUF","text":"<pre><code>flowchart TB\n    subgraph MODEL[\"TRANSFORMER MODEL TENSORS\"]\n        direction TB\n\n        subgraph EMB[\"EMBEDDINGS\"]\n            TE[\"token_embd.weight\"]\n        end\n\n        subgraph BLOCKS[\"TRANSFORMER BLOCKS \u00d7 N\"]\n            subgraph ATTN[\"ATTENTION\"]\n                QW[\"blk.N.attn_q.weight\"]\n                KW[\"blk.N.attn_k.weight\"]\n                VW[\"blk.N.attn_v.weight\"]\n                OW[\"blk.N.attn_output.weight\"]\n            end\n\n            subgraph FFN[\"FEED FORWARD\"]\n                G[\"blk.N.ffn_gate.weight\"]\n                U[\"blk.N.ffn_up.weight\"]\n                D[\"blk.N.ffn_down.weight\"]\n            end\n\n            subgraph NORM[\"LAYER NORMS\"]\n                AN[\"blk.N.attn_norm.weight\"]\n                FN[\"blk.N.ffn_norm.weight\"]\n            end\n        end\n\n        subgraph OUT[\"OUTPUT\"]\n            ON[\"output_norm.weight\"]\n            OL[\"output.weight\"]\n        end\n    end\n\n    EMB --&gt; BLOCKS --&gt; OUT\n\n    style TE fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style OL fill:#48bb78,stroke:#2f855a,color:#fff</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#reading-gguf-in-python","title":"Reading GGUF in Python","text":"<p>Here's a minimal example for reading GGUF headers on Windows (CPU only):</p> <pre><code>import struct\n\ndef read_gguf_header(filepath):\n    \"\"\"Read GGUF file header information.\"\"\"\n    with open(filepath, 'rb') as f:\n        # Read magic\n        magic = f.read(4)\n        if magic != b'GGUF':\n            raise ValueError(f\"Invalid GGUF magic: {magic}\")\n\n        # Read version (uint32, little-endian)\n        version = struct.unpack('&lt;I', f.read(4))[0]\n\n        # Read tensor count (uint64, little-endian)\n        tensor_count = struct.unpack('&lt;Q', f.read(8))[0]\n\n        # Read metadata kv count (uint64, little-endian)\n        metadata_kv_count = struct.unpack('&lt;Q', f.read(8))[0]\n\n        return {\n            'magic': magic.decode('ascii'),\n            'version': version,\n            'tensor_count': tensor_count,\n            'metadata_kv_count': metadata_kv_count\n        }\n\n# Usage\n# header = read_gguf_header(\"model.gguf\")\n# print(header)\n</code></pre>"},{"location":"LLM/gguf-file-structure-guide/#tools-for-working-with-gguf","title":"Tools for Working with GGUF","text":"Tool Purpose Platform <code>llama.cpp</code> Inference engine Windows/Linux/Mac <code>gguf-py</code> Python GGUF library Cross-platform <code>llama-quantize</code> Quantization tool Windows/Linux/Mac <code>llama-gguf</code> GGUF manipulation Windows/Linux/Mac"},{"location":"LLM/gguf-file-structure-guide/#summary","title":"Summary","text":"<pre><code>flowchart TB\n    subgraph SUMMARY[\"GGUF FORMAT SUMMARY\"]\n        direction LR\n\n        S1[\"\ud83d\udd12 Self-Contained&lt;br/&gt;Single file distribution\"]\n        S2[\"\u26a1 Memory Mapped&lt;br/&gt;Fast loading\"]\n        S3[\"\ud83d\udcca Quantized&lt;br/&gt;Reduced size\"]\n        S4[\"\ud83d\udccb Metadata Rich&lt;br/&gt;Complete model info\"]\n    end\n\n    style S1 fill:#4299e1,stroke:#2b6cb0,color:#fff\n    style S2 fill:#48bb78,stroke:#2f855a,color:#fff\n    style S3 fill:#ed8936,stroke:#c05621,color:#fff\n    style S4 fill:#9f7aea,stroke:#6b46c1,color:#fff</code></pre> <p>Key Takeaways:</p> <ol> <li>Header (24 bytes): Magic number, version, counts</li> <li>Metadata: Key-value pairs for model configuration and tokenizer</li> <li>Tensor Info: Names, dimensions, types, and offsets for each tensor</li> <li>Tensor Data: Aligned, quantized weight data</li> </ol> <p>GGUF enables efficient distribution and inference of large language models on consumer hardware, including CPU-only systems like your Lenovo Yoga with 32GB RAM.</p> <p>Guide generated for GGUF format version 3</p>"},{"location":"LLM/safetensors_file_structure/","title":"Safetensors File Format Structure","text":"<p>The <code>.safetensors</code> format is a simple, fast, and safe file format created by Hugging Face for storing tensors. It was designed to address security vulnerabilities in pickle-based formats (like PyTorch's <code>.pt</code> files) while being extremely fast to load.</p>"},{"location":"LLM/safetensors_file_structure/#high-level-structure","title":"High-Level Structure","text":"<p>A safetensors file consists of exactly three sequential regions:</p> <pre><code>graph LR\n    subgraph \"Safetensors File Layout\"\n        A[\"Header Size&lt;br/&gt;(8 bytes)\"] --&gt; B[\"JSON Header&lt;br/&gt;(variable size)\"]\n        B --&gt; C[\"Tensor Data&lt;br/&gt;(raw bytes)\"]\n    end\n\n    style A fill:#e74c3c,color:#fff\n    style B fill:#3498db,color:#fff\n    style C fill:#27ae60,color:#fff</code></pre>"},{"location":"LLM/safetensors_file_structure/#1-header-size-bytes-07","title":"1. Header Size (Bytes 0\u20137)","text":"<p>The first 8 bytes store a little-endian unsigned 64-bit integer indicating the size of the JSON header in bytes.</p> <pre><code>graph TB\n    subgraph \"Header Size Field (8 bytes)\"\n        direction LR\n        B0[\"Byte 0&lt;br/&gt;LSB\"] --- B1[\"Byte 1\"] --- B2[\"Byte 2\"] --- B3[\"Byte 3\"] --- B4[\"Byte 4\"] --- B5[\"Byte 5\"] --- B6[\"Byte 6\"] --- B7[\"Byte 7&lt;br/&gt;MSB\"]\n    end\n\n    Note[\"Example: If header is 256 bytes&lt;br/&gt;Stored as: 0x00 0x01 0x00 0x00 0x00 0x00 0x00 0x00\"]\n\n    style B0 fill:#e74c3c,color:#fff\n    style B7 fill:#e74c3c,color:#fff</code></pre> <p>Key Points: - Little-endian byte order (least significant byte first) - Maximum theoretical header size: 2^64 bytes (practically limited by file system) - This fixed-size prefix enables fast header parsing</p>"},{"location":"LLM/safetensors_file_structure/#2-json-header-bytes-8-to-8n","title":"2. JSON Header (Bytes 8 to 8+N)","text":"<p>The header is a UTF-8 encoded JSON object containing metadata for all tensors and optional file-level metadata.</p> <pre><code>graph TB\n    subgraph \"JSON Header Structure\"\n        ROOT[\"{ }\"]\n        ROOT --&gt; META[\"__metadata__&lt;br/&gt;(optional)\"]\n        ROOT --&gt; T1[\"tensor_name_1\"]\n        ROOT --&gt; T2[\"tensor_name_2\"]\n        ROOT --&gt; TN[\"...more tensors\"]\n\n        META --&gt; M1[\"key: value pairs&lt;br/&gt;(all strings)\"]\n\n        T1 --&gt; D1[\"dtype\"]\n        T1 --&gt; S1[\"shape\"]\n        T1 --&gt; O1[\"data_offsets\"]\n\n        D1 --&gt; DV1[\"'F32' | 'F16' | 'BF16' | 'I32' | ...\"]\n        S1 --&gt; SV1[\"[dim0, dim1, ...]\"]\n        O1 --&gt; OV1[\"[start, end]\"]\n    end\n\n    style ROOT fill:#3498db,color:#fff\n    style META fill:#9b59b6,color:#fff\n    style T1 fill:#f39c12,color:#fff\n    style T2 fill:#f39c12,color:#fff\n    style TN fill:#f39c12,color:#fff</code></pre>"},{"location":"LLM/safetensors_file_structure/#tensor-entry-fields","title":"Tensor Entry Fields","text":"Field Type Description <code>dtype</code> string Data type of tensor elements <code>shape</code> array of integers Dimensions of the tensor <code>data_offsets</code> [start, end] Byte range in data region (exclusive end)"},{"location":"LLM/safetensors_file_structure/#header-json-example","title":"Header JSON Example","text":"<pre><code>{\n  \"__metadata__\": {\n    \"format\": \"pt\",\n    \"author\": \"huggingface\",\n    \"description\": \"Example model weights\"\n  },\n  \"model.embed.weight\": {\n    \"dtype\": \"F16\",\n    \"shape\": [50257, 768],\n    \"data_offsets\": [0, 77194752]\n  },\n  \"model.layer.0.attn.weight\": {\n    \"dtype\": \"F16\",\n    \"shape\": [768, 768],\n    \"data_offsets\": [77194752, 78373888]\n  },\n  \"model.layer.0.attn.bias\": {\n    \"dtype\": \"F16\",\n    \"shape\": [768],\n    \"data_offsets\": [78373888, 78375424]\n  }\n}\n</code></pre>"},{"location":"LLM/safetensors_file_structure/#the-__metadata__-section","title":"The <code>__metadata__</code> Section","text":"<ul> <li>Optional section for arbitrary file-level metadata</li> <li>All values must be strings (not numbers or objects)</li> <li>Common uses: format version, author, training info, license</li> </ul>"},{"location":"LLM/safetensors_file_structure/#3-tensor-data-region-bytes-8n-onwards","title":"3. Tensor Data Region (Bytes 8+N onwards)","text":"<p>Raw tensor bytes stored contiguously. Each tensor's location is defined by <code>data_offsets</code> relative to the start of the data region (not the file).</p> <pre><code>graph TB\n    subgraph \"Data Region Layout\"\n        direction LR\n        T1[\"Tensor 1&lt;br/&gt;bytes [0, end1)\"]\n        T2[\"Tensor 2&lt;br/&gt;bytes [end1, end2)\"]\n        T3[\"Tensor 3&lt;br/&gt;bytes [end2, end3)\"]\n        TN[\"...\"]\n\n        T1 --&gt; T2 --&gt; T3 --&gt; TN\n    end\n\n    subgraph \"Offset Mapping\"\n        O1[\"data_offsets: [0, 1024]\"] -.-&gt; T1\n        O2[\"data_offsets: [1024, 5120]\"] -.-&gt; T2\n        O3[\"data_offsets: [5120, 9216]\"] -.-&gt; T3\n    end\n\n    style T1 fill:#27ae60,color:#fff\n    style T2 fill:#27ae60,color:#fff\n    style T3 fill:#27ae60,color:#fff</code></pre>"},{"location":"LLM/safetensors_file_structure/#calculating-tensor-size","title":"Calculating Tensor Size","text":"<p>The size of each tensor in bytes can be calculated as:</p> <pre><code>tensor_size = data_offsets[1] - data_offsets[0]\n</code></pre> <p>Or verified via shape and dtype:</p> <pre><code>tensor_size = product(shape) \u00d7 bytes_per_element(dtype)\n</code></pre>"},{"location":"LLM/safetensors_file_structure/#complete-file-layout-with-byte-addresses","title":"Complete File Layout with Byte Addresses","text":"<pre><code>graph TB\n    subgraph \"Complete Safetensors File\"\n        direction TB\n\n        subgraph \"Address 0x0000\"\n            HS[\"HEADER SIZE&lt;br/&gt;8 bytes, uint64 LE&lt;br/&gt;Value: N\"]\n        end\n\n        subgraph \"Address 0x0008\"\n            HD[\"JSON HEADER&lt;br/&gt;N bytes, UTF-8&lt;br/&gt;Tensor metadata + optional __metadata__\"]\n        end\n\n        subgraph \"Address 0x0008 + N\"\n            DATA[\"TENSOR DATA REGION&lt;br/&gt;Raw bytes, contiguous&lt;br/&gt;Offsets are relative to this point\"]\n        end\n    end\n\n    HS --&gt; HD --&gt; DATA\n\n    style HS fill:#e74c3c,color:#fff\n    style HD fill:#3498db,color:#fff\n    style DATA fill:#27ae60,color:#fff</code></pre>"},{"location":"LLM/safetensors_file_structure/#address-calculation-formula","title":"Address Calculation Formula","text":"<p>To find the absolute file position of a tensor:</p> <pre><code>absolute_position = 8 + header_size + data_offsets[0]\n</code></pre>"},{"location":"LLM/safetensors_file_structure/#supported-data-types","title":"Supported Data Types","text":"dtype String Description Bytes per Element NumPy Equivalent <code>F64</code> 64-bit float 8 <code>float64</code> <code>F32</code> 32-bit float 4 <code>float32</code> <code>F16</code> 16-bit float 2 <code>float16</code> <code>BF16</code> Brain float 16 2 N/A (use <code>bfloat16</code>) <code>I64</code> 64-bit signed int 8 <code>int64</code> <code>I32</code> 32-bit signed int 4 <code>int32</code> <code>I16</code> 16-bit signed int 2 <code>int16</code> <code>I8</code> 8-bit signed int 1 <code>int8</code> <code>U8</code> 8-bit unsigned int 1 <code>uint8</code> <code>BOOL</code> Boolean 1 <code>bool</code>"},{"location":"LLM/safetensors_file_structure/#memory-mapping-advantage","title":"Memory Mapping Advantage","text":"<pre><code>flowchart LR\n    subgraph \"Traditional Loading\"\n        A1[Read File] --&gt; A2[Parse All] --&gt; A3[Allocate RAM] --&gt; A4[Copy to RAM]\n    end\n\n    subgraph \"Safetensors Memory Mapping\"\n        B1[Open File] --&gt; B2[Parse Header Only] --&gt; B3[mmap Data Region] --&gt; B4[Access on Demand]\n    end\n\n    A4 --&gt; SLOW[\"\u274c Slow, High RAM\"]\n    B4 --&gt; FAST[\"\u2705 Fast, Low RAM\"]\n\n    style SLOW fill:#e74c3c,color:#fff\n    style FAST fill:#27ae60,color:#fff</code></pre>"},{"location":"LLM/safetensors_file_structure/#why-memory-mapping-works","title":"Why Memory Mapping Works","text":"<p>The key design benefit: since offsets point directly into contiguous raw data, the entire data region can be memory-mapped. </p> <p>Benefits include: - Fast startup: Only header is parsed initially - Lazy loading: Tensor bytes loaded only when accessed - Shared memory: Multiple processes can share the same mapped region - Reduced RAM: OS manages page swapping automatically</p>"},{"location":"LLM/safetensors_file_structure/#why-its-safe","title":"Why It's \"Safe\"","text":"<pre><code>graph TB\n    subgraph \"Security Comparison\"\n        direction LR\n\n        subgraph \"Pickle (.pt, .pkl)\"\n            P1[\"Arbitrary Python Code\"] --&gt; P2[\"Can Execute on Load\"] --&gt; P3[\"\ud83d\udd34 Remote Code Execution Risk\"]\n        end\n\n        subgraph \"Safetensors\"\n            S1[\"JSON + Raw Bytes Only\"] --&gt; S2[\"No Code Execution\"] --&gt; S3[\"\ud83d\udfe2 Safe to Load\"]\n        end\n    end\n\n    style P3 fill:#e74c3c,color:#fff\n    style S3 fill:#27ae60,color:#fff</code></pre>"},{"location":"LLM/safetensors_file_structure/#security-features","title":"Security Features","text":"<ol> <li>No code execution: Contains only JSON and raw numeric data</li> <li>Bounded reads: Header size prevents buffer overflow attacks</li> <li>Validation: Offsets can be verified before accessing data</li> <li>Deterministic parsing: No deserialization of arbitrary objects</li> </ol>"},{"location":"LLM/safetensors_file_structure/#summary-table","title":"Summary Table","text":"Region Start Address Size Content Header Size <code>0x0000</code> 8 bytes Little-endian uint64 JSON Header <code>0x0008</code> N bytes (from header size) UTF-8 JSON with tensor metadata Tensor Data <code>0x0008 + N</code> Remaining file Raw contiguous tensor bytes"},{"location":"LLM/safetensors_file_structure/#reading-a-safetensors-file-pseudocode","title":"Reading a Safetensors File (Pseudocode)","text":"<pre><code>def read_safetensors(filepath):\n    with open(filepath, 'rb') as f:\n        # 1. Read header size (first 8 bytes)\n        header_size = int.from_bytes(f.read(8), byteorder='little')\n\n        # 2. Read and parse JSON header\n        header_json = f.read(header_size).decode('utf-8')\n        header = json.loads(header_json)\n\n        # 3. Data region starts at offset 8 + header_size\n        data_start = 8 + header_size\n\n        # 4. Load specific tensor\n        tensor_info = header['model.embed.weight']\n        start, end = tensor_info['data_offsets']\n\n        f.seek(data_start + start)\n        raw_bytes = f.read(end - start)\n\n        # 5. Convert to numpy array\n        dtype_map = {'F32': np.float32, 'F16': np.float16, ...}\n        array = np.frombuffer(raw_bytes, dtype=dtype_map[tensor_info['dtype']])\n        array = array.reshape(tensor_info['shape'])\n\n        return array\n</code></pre>"},{"location":"LLM/safetensors_file_structure/#references","title":"References","text":"<ul> <li>Hugging Face Safetensors Repository</li> <li>Safetensors Documentation</li> </ul>"},{"location":"LLM/transformer-weights-explained/","title":"Transformer Weights Explained: What They Actually Are","text":""},{"location":"LLM/transformer-weights-explained/#common-misconception","title":"Common Misconception","text":"<p>A common misconception is that weights represent the \"value of a path from one token to another.\" This is incorrect.</p> <p>Weights are learned numerical parameters stored in matrices (tensors). They don't represent direct \"paths between tokens\" \u2014 instead, they define transformations that the model applies to convert inputs into outputs through many layers.</p> <pre><code>Weights \u2260 \"Token A \u2192 Token B = 0.7\"\n\nWeights = Matrices that transform vectors through mathematical operations\n</code></pre>"},{"location":"LLM/transformer-weights-explained/#how-tokens-actually-flow-through-the-model","title":"How Tokens Actually Flow Through the Model","text":"<pre><code>flowchart TB\n    subgraph INPUT[\"INPUT\"]\n        T1[\"Token: 'Hello'\"]\n        T2[\"Token: 'world'\"]\n    end\n\n    subgraph EMB[\"EMBEDDING LAYER\"]\n        E1[\"Vector: [0.12, -0.45, 0.89, ...]&lt;br/&gt;4096 dimensions\"]\n        E2[\"Vector: [0.34, 0.21, -0.56, ...]&lt;br/&gt;4096 dimensions\"]\n    end\n\n    subgraph TRANSFORM[\"TRANSFORMER LAYERS \u00d7 N\"]\n        ATT[\"Attention&lt;br/&gt;(weights transform queries, keys, values)\"]\n        FFN[\"Feed-Forward Network&lt;br/&gt;(weights transform representations)\"]\n    end\n\n    subgraph OUTPUT[\"OUTPUT\"]\n        LOGITS[\"Logits: probability scores&lt;br/&gt;for ALL vocab tokens\"]\n        NEXT[\"Next token: '!'&lt;br/&gt;(highest probability)\"]\n    end\n\n    T1 --&gt; E1\n    T2 --&gt; E2\n    E1 &amp; E2 --&gt; ATT --&gt; FFN --&gt; LOGITS --&gt; NEXT\n\n    style EMB fill:#4a90d9,stroke:#2c5282,color:#fff\n    style TRANSFORM fill:#48bb78,stroke:#276749,color:#fff\n    style LOGITS fill:#ed8936,stroke:#c05621,color:#fff</code></pre>"},{"location":"LLM/transformer-weights-explained/#the-key-weight-tensors-in-a-transformer","title":"The Key Weight Tensors in a Transformer","text":"<p>Each tensor serves a specific mathematical purpose:</p>"},{"location":"LLM/transformer-weights-explained/#1-token-embeddings-token_embdweight","title":"1. Token Embeddings (<code>token_embd.weight</code>)","text":"<pre><code>Shape: [vocab_size \u00d7 embedding_dim]\nExample: [32000 \u00d7 4096]\n</code></pre> <ul> <li>Maps each token ID to a dense vector</li> <li>Token 1547 (\"Hello\") \u2192 look up row 1547 \u2192 get 4096-dimensional vector</li> <li>This is a lookup table, not a path</li> </ul>"},{"location":"LLM/transformer-weights-explained/#2-attention-weights-per-layer","title":"2. Attention Weights (per layer)","text":"<pre><code>Q weight: [hidden_dim \u00d7 hidden_dim]  \u2014 transforms input to \"Query\"\nK weight: [hidden_dim \u00d7 hidden_dim]  \u2014 transforms input to \"Key\"  \nV weight: [hidden_dim \u00d7 hidden_dim]  \u2014 transforms input to \"Value\"\nO weight: [hidden_dim \u00d7 hidden_dim]  \u2014 projects attention output\n</code></pre> <ul> <li>These matrices learn what to pay attention to</li> <li>Attention scores between tokens are computed at runtime, not stored</li> </ul>"},{"location":"LLM/transformer-weights-explained/#3-feed-forward-weights-per-layer","title":"3. Feed-Forward Weights (per layer)","text":"<pre><code>ffn_up:   [hidden_dim \u00d7 intermediate_dim]\nffn_down: [intermediate_dim \u00d7 hidden_dim]\nffn_gate: [hidden_dim \u00d7 intermediate_dim]  (for gated architectures)\n</code></pre> <ul> <li>Transform the representation through a non-linear \"thinking\" step</li> </ul>"},{"location":"LLM/transformer-weights-explained/#4-output-layer-outputweight","title":"4. Output Layer (<code>output.weight</code>)","text":"<pre><code>Shape: [embedding_dim \u00d7 vocab_size]\nExample: [4096 \u00d7 32000]\n</code></pre> <ul> <li>Converts final hidden state \u2192 scores for every possible next token</li> </ul>"},{"location":"LLM/transformer-weights-explained/#analogy-weights-as-lenses","title":"Analogy: Weights as \"Lenses\"","text":"<pre><code>flowchart LR\n    subgraph WRONG[\"\u274c WRONG MENTAL MODEL\"]\n        A1[Token A] --&gt;|\"weight = 0.7\"| B1[Token B]\n        A1 --&gt;|\"weight = 0.2\"| C1[Token C]\n    end\n\n    subgraph RIGHT[\"\u2705 CORRECT MENTAL MODEL\"]\n        IN[\"Input Vector&lt;br/&gt;[0.1, 0.5, -0.3, ...]\"]\n        W[\"Weight Matrix&lt;br/&gt;(learned lens)\"]\n        OUT[\"Output Vector&lt;br/&gt;[0.8, -0.2, 0.4, ...]\"]\n        IN --&gt;|\"matrix multiply\"| W --&gt;|\"produces\"| OUT\n    end</code></pre> <p>Weights are like lenses that transform the meaning of input vectors. The model learns during training how to shape these lenses so that, after many transformations, the final output correctly predicts the next token.</p>"},{"location":"LLM/transformer-weights-explained/#what-the-tokentoken-model-might-be-confused-with","title":"What the \"Token\u2192Token\" Model Might Be Confused With","text":"Concept What It Is N-gram models Older models that did store direct token\u2192token probabilities Attention scores Computed at runtime (not stored) \u2014 how much each token attends to others Graph neural networks Where edge weights connect nodes directly <p>In transformers, the \"relationship\" between tokens is computed dynamically through matrix operations using the stored weights \u2014 it's not pre-stored as a lookup table.</p>"},{"location":"LLM/transformer-weights-explained/#matrix-multiplication-the-core-operation","title":"Matrix Multiplication: The Core Operation","text":"<p>Every layer applies weights through matrix multiplication:</p> <pre><code>flowchart LR\n    subgraph MATMUL[\"MATRIX MULTIPLICATION\"]\n        INPUT[\"Input Vector&lt;br/&gt;[1 \u00d7 4096]\"]\n        WEIGHT[\"Weight Matrix&lt;br/&gt;[4096 \u00d7 4096]\"]\n        OUTPUT[\"Output Vector&lt;br/&gt;[1 \u00d7 4096]\"]\n\n        INPUT --&gt; WEIGHT --&gt; OUTPUT\n    end</code></pre> <p>Mathematical form:</p> <pre><code>output = input \u00d7 weight_matrix + bias\n</code></pre> <p>Each element in the output is a weighted sum of all input elements. The weights determine how to combine the input values.</p>"},{"location":"LLM/transformer-weights-explained/#example-a-single-attention-head","title":"Example: A Single Attention Head","text":"<pre><code>flowchart TB\n    subgraph ATTENTION[\"SINGLE ATTENTION HEAD\"]\n        direction TB\n\n        X[\"Input X&lt;br/&gt;(sequence of vectors)\"]\n\n        subgraph PROJ[\"PROJECTIONS (using stored weights)\"]\n            Q[\"Q = X \u00d7 W_q\"]\n            K[\"K = X \u00d7 W_k\"]\n            V[\"V = X \u00d7 W_v\"]\n        end\n\n        SCORES[\"Attention Scores&lt;br/&gt;softmax(Q \u00d7 K^T / \u221ad)&lt;br/&gt;&lt;em&gt;computed at runtime&lt;/em&gt;\"]\n\n        OUT[\"Output = Scores \u00d7 V\"]\n    end\n\n    X --&gt; Q &amp; K &amp; V\n    Q &amp; K --&gt; SCORES\n    SCORES &amp; V --&gt; OUT\n\n    style PROJ fill:#4a90d9,stroke:#2c5282,color:#fff\n    style SCORES fill:#ed8936,stroke:#c05621,color:#fff</code></pre> <p>Key insight: The weights (<code>W_q</code>, <code>W_k</code>, <code>W_v</code>) are stored, but the attention scores (which tokens attend to which) are computed fresh for every input.</p>"},{"location":"LLM/transformer-weights-explained/#stored-vs-computed","title":"Stored vs. Computed","text":"Stored in GGUF Weights Computed at Runtime Weight matrices (parameters) Token-to-token attention scores Token embeddings (lookup table) Attention patterns Layer normalization values Intermediate activations Bias terms Probability distributions"},{"location":"LLM/transformer-weights-explained/#why-quantization-works","title":"Why Quantization Works","text":"<p>Since weights are just numbers in matrices, we can compress them:</p> Precision Bits per Weight Memory for 7B Model FP32 32 bits ~28 GB FP16 16 bits ~14 GB Q8_0 8 bits ~7 GB Q4_K ~4.5 bits ~4 GB Q2_K ~2.5 bits ~2.5 GB <p>Quantization trades precision for size. The model still performs the same operations, just with less precise numbers.</p>"},{"location":"LLM/transformer-weights-explained/#summary","title":"Summary","text":"<p>The magic of LLMs is that billions of weight values, when combined through matrix multiplications across many layers, produce emergent behavior that appears to understand token relationships \u2014 but those relationships are computed, not stored.</p> <pre><code>flowchart TB\n    subgraph SUMMARY[\"KEY TAKEAWAY\"]\n        W[\"Billions of Weights&lt;br/&gt;(stored parameters)\"]\n        M[\"Matrix Operations&lt;br/&gt;(at runtime)\"]\n        E[\"Emergent Understanding&lt;br/&gt;(appears intelligent)\"]\n\n        W --&gt;|\"combined through\"| M --&gt;|\"produces\"| E\n    end\n\n    style W fill:#9f7aea,stroke:#6b46c1,color:#fff\n    style M fill:#ed8936,stroke:#c05621,color:#fff\n    style E fill:#48bb78,stroke:#276749,color:#fff</code></pre> <p>Guide created to clarify transformer weight mechanics</p>"},{"location":"LLM/why_models_have_multiple_tensors/","title":"Why AI Models Consist of Multiple Tensors","text":"<p>This document explains why a <code>.safetensors</code> file representing an AI model is composed of many distinct tensors rather than one giant tensor.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#the-core-reason-neural-networks-are-graphs-of-operations","title":"The Core Reason: Neural Networks Are Graphs of Operations","text":"<p>A neural network isn't a single mathematical operation\u2014it's a directed graph of many distinct operations, each with its own learned parameters.</p> <pre><code>graph LR\n    subgraph \"Simple Neural Network\"\n        I[Input] --&gt; L1[\"Layer 1&lt;br/&gt;weights + bias\"]\n        L1 --&gt; A1[Activation]\n        A1 --&gt; L2[\"Layer 2&lt;br/&gt;weights + bias\"]\n        L2 --&gt; A2[Activation]\n        A2 --&gt; L3[\"Layer 3&lt;br/&gt;weights + bias\"]\n        L3 --&gt; O[Output]\n    end\n\n    style L1 fill:#3498db,color:#fff\n    style L2 fill:#3498db,color:#fff\n    style L3 fill:#3498db,color:#fff</code></pre> <p>Each blue box represents separate tensors with different shapes and purposes.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#reason-1-different-shapes-for-different-operations","title":"Reason 1: Different Shapes for Different Operations","text":"<p>Each layer performs a specific mathematical operation requiring a specific tensor shape.</p> <pre><code>graph TB\n    subgraph \"Transformer Block - Different Tensor Shapes\"\n        EMB[\"Embedding&lt;br/&gt;[vocab_size \u00d7 hidden_dim]&lt;br/&gt;50257 \u00d7 768\"]\n\n        QKV[\"Q, K, V Projections&lt;br/&gt;[hidden \u00d7 hidden] each&lt;br/&gt;768 \u00d7 768\"]\n\n        ATT_OUT[\"Attention Output&lt;br/&gt;[hidden \u00d7 hidden]&lt;br/&gt;768 \u00d7 768\"]\n\n        FFN1[\"FFN Layer 1&lt;br/&gt;[hidden \u00d7 4\u00d7hidden]&lt;br/&gt;768 \u00d7 3072\"]\n\n        FFN2[\"FFN Layer 2&lt;br/&gt;[4\u00d7hidden \u00d7 hidden]&lt;br/&gt;3072 \u00d7 768\"]\n\n        LN[\"Layer Norm&lt;br/&gt;[hidden]&lt;br/&gt;768\"]\n    end\n\n    EMB --&gt; QKV --&gt; ATT_OUT --&gt; FFN1 --&gt; FFN2\n    LN -.-&gt; |\"Applied at multiple points\"| QKV\n\n    style EMB fill:#e74c3c,color:#fff\n    style QKV fill:#3498db,color:#fff\n    style FFN1 fill:#27ae60,color:#fff\n    style FFN2 fill:#27ae60,color:#fff\n    style LN fill:#9b59b6,color:#fff</code></pre> <p>You cannot combine a [50257 \u00d7 768] tensor with a [768 \u00d7 768] tensor into one\u2014they have incompatible shapes and serve different purposes.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#reason-2-weights-vs-biases","title":"Reason 2: Weights vs. Biases","text":"<p>Even within a single layer, weights and biases are separate tensors:</p> <pre><code>graph LR\n    subgraph \"Single Linear Layer: y = Wx + b\"\n        X[\"Input x&lt;br/&gt;[batch, 768]\"]\n        W[\"Weight W&lt;br/&gt;[768, 768]&lt;br/&gt;2D tensor\"]\n        B[\"Bias b&lt;br/&gt;[768]&lt;br/&gt;1D tensor\"]\n        Y[\"Output y&lt;br/&gt;[batch, 768]\"]\n\n        X --&gt; |\"matrix multiply\"| MUL[\"\u00d7\"]\n        W --&gt; MUL\n        MUL --&gt; ADD[\"+\"]\n        B --&gt; ADD\n        ADD --&gt; Y\n    end\n\n    style W fill:#3498db,color:#fff\n    style B fill:#e74c3c,color:#fff</code></pre> <p>The weight is a 2D matrix, the bias is a 1D vector. Different shapes = different tensors.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#reason-3-modularity-and-flexibility","title":"Reason 3: Modularity and Flexibility","text":"<p>Separate tensors enable:</p> Capability How Separate Tensors Help Fine-tuning Freeze some layers, train others Transfer learning Replace just the final layer LoRA adapters Add small tensors alongside existing ones Pruning Remove individual layers or heads Quantization Apply different precision to different layers Inspection Analyze specific layer behaviors <pre><code>graph TB\n    subgraph \"Fine-Tuning Example\"\n        direction TB\n        FROZEN[\"Frozen Layers&lt;br/&gt;(loaded, not trained)\"]\n        TRAINABLE[\"New Classification Head&lt;br/&gt;(randomly initialized, trained)\"]\n\n        FROZEN --&gt; TRAINABLE\n    end\n\n    style FROZEN fill:#95a5a6,color:#fff\n    style TRAINABLE fill:#27ae60,color:#fff</code></pre> <p>If everything were one tensor, you couldn't selectively freeze or modify parts.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#reason-4-memory-efficiency-during-computation","title":"Reason 4: Memory Efficiency During Computation","text":"<p>During inference, not all tensors are needed simultaneously:</p> <pre><code>sequenceDiagram\n    participant RAM as System RAM\n    participant GPU as GPU Memory\n    participant Compute as Computation\n\n    Note over RAM: All tensors stored\n\n    RAM-&gt;&gt;GPU: Load Layer 1 weights\n    GPU-&gt;&gt;Compute: Process Layer 1\n    GPU--&gt;&gt;RAM: Optionally offload\n\n    RAM-&gt;&gt;GPU: Load Layer 2 weights\n    GPU-&gt;&gt;Compute: Process Layer 2\n    GPU--&gt;&gt;RAM: Optionally offload\n\n    Note over GPU: Only active layer&lt;br/&gt;needs GPU memory</code></pre> <p>Separate tensors allow layer-by-layer streaming for models larger than GPU memory.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#reason-5-a-real-models-tensor-inventory","title":"Reason 5: A Real Model's Tensor Inventory","text":"<p>A typical transformer model contains these tensor categories:</p> <pre><code>graph TB\n    subgraph \"GPT-2 Style Model Tensors\"\n        subgraph \"Embeddings\"\n            WTE[\"wte.weight&lt;br/&gt;Token embeddings&lt;br/&gt;[50257, 768]\"]\n            WPE[\"wpe.weight&lt;br/&gt;Position embeddings&lt;br/&gt;[1024, 768]\"]\n        end\n\n        subgraph \"Per-Layer Tensors (\u00d712 layers)\"\n            LN1[\"ln_1.weight, ln_1.bias&lt;br/&gt;[768] each\"]\n            ATT[\"attn.c_attn.weight&lt;br/&gt;[768, 2304]\"]\n            ATTB[\"attn.c_attn.bias&lt;br/&gt;[2304]\"]\n            PROJ[\"attn.c_proj.weight&lt;br/&gt;[768, 768]\"]\n            LN2[\"ln_2.weight, ln_2.bias&lt;br/&gt;[768] each\"]\n            MLP1[\"mlp.c_fc.weight&lt;br/&gt;[768, 3072]\"]\n            MLP2[\"mlp.c_proj.weight&lt;br/&gt;[3072, 768]\"]\n        end\n\n        subgraph \"Final\"\n            LNF[\"ln_f.weight, ln_f.bias&lt;br/&gt;[768] each\"]\n        end\n    end\n\n    style WTE fill:#e74c3c,color:#fff\n    style WPE fill:#e74c3c,color:#fff\n    style ATT fill:#3498db,color:#fff\n    style MLP1 fill:#27ae60,color:#fff\n    style MLP2 fill:#27ae60,color:#fff</code></pre> <p>GPT-2 Small has 148 separate tensors. GPT-2 XL has 292 tensors. LLaMA 70B has thousands.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#why-not-flatten-everything-into-one-tensor","title":"Why Not Flatten Everything Into One Tensor?","text":"<p>You could technically concatenate all parameters into a single 1D tensor:</p> <pre><code>graph LR\n    subgraph \"Hypothetical Single Tensor\"\n        FLAT[\"[param_0, param_1, param_2, ..., param_N]&lt;br/&gt;One giant 1D array\"]\n    end\n\n    subgraph \"Problems\"\n        P1[\"\u274c Lose shape information\"]\n        P2[\"\u274c Can't do matrix operations directly\"]\n        P3[\"\u274c Must reshape constantly\"]\n        P4[\"\u274c Can't load partially\"]\n        P5[\"\u274c Can't apply different dtypes\"]\n    end\n\n    FLAT --&gt; P1\n    FLAT --&gt; P2\n    FLAT --&gt; P3\n    FLAT --&gt; P4\n    FLAT --&gt; P5\n\n    style FLAT fill:#e74c3c,color:#fff</code></pre> <p>You'd need a separate metadata structure to track where each layer's parameters begin and end, what shape to reshape them into, etc.\u2014essentially recreating what safetensors already does, but less efficiently.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#the-mathematical-perspective","title":"The Mathematical Perspective","text":"<p>A neural network computes a function by composing many smaller functions:</p> <pre><code>output = f_n( f_{n-1}( ... f_2( f_1( input ) ) ... ) )\n</code></pre> <p>Each function <code>f_i</code> typically has the form:</p> <pre><code>f_i(x) = activation( W_i \u00b7 x + b_i )\n</code></pre> <p>Where: - <code>W_i</code> is a weight matrix (2D tensor) - <code>b_i</code> is a bias vector (1D tensor) - Each layer has its own <code>W</code> and <code>b</code></p> <p>This mathematical structure inherently requires multiple separate parameter tensors.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#analogy-a-car-vs-its-parts","title":"Analogy: A Car vs. Its Parts","text":"<p>Think of asking \"why does a car have multiple parts instead of one?\"</p> <pre><code>graph TB\n    subgraph \"A Car's Components\"\n        ENGINE[\"Engine&lt;br/&gt;(converts fuel to motion)\"]\n        TRANS[\"Transmission&lt;br/&gt;(transfers power)\"]\n        WHEELS[\"Wheels&lt;br/&gt;(contact ground)\"]\n        STEERING[\"Steering&lt;br/&gt;(direction control)\"]\n        BRAKES[\"Brakes&lt;br/&gt;(stopping)\"]\n    end\n\n    ENGINE --&gt; TRANS --&gt; WHEELS\n    STEERING --&gt; WHEELS\n    BRAKES --&gt; WHEELS\n\n    style ENGINE fill:#e74c3c,color:#fff\n    style TRANS fill:#3498db,color:#fff\n    style WHEELS fill:#27ae60,color:#fff</code></pre> <p>Each component: - Has a different shape (you can't combine an engine and a wheel) - Serves a different function - Can be replaced independently - Must be connected in a specific way</p> <p>Neural network tensors are the same\u2014each serves a specific purpose in the computation.</p>"},{"location":"LLM/why_models_have_multiple_tensors/#summary","title":"Summary","text":"Reason Explanation Mathematical necessity Different operations require different tensor shapes Architectural structure Networks are graphs of distinct operations Weights vs. biases Even one layer has multiple parameter tensors Modularity Enables fine-tuning, pruning, quantization Memory management Allows streaming and partial loading Framework design PyTorch/TensorFlow organize models as named parameter collections"},{"location":"LLM/why_models_have_multiple_tensors/#conclusion","title":"Conclusion","text":"<p>The multiple-tensor design directly mirrors the mathematical structure of neural networks. Each tensor represents a learnable parameter matrix or vector for a specific operation in the computation graph.</p> <p>A single-tensor design would: 1. Lose the natural correspondence between tensors and operations 2. Require constant reshaping during computation 3. Prevent partial loading and fine-tuning 4. Make the model file format more complex, not simpler</p> <p>The safetensors format (and all model formats) use multiple named tensors because that's what neural networks fundamentally are: collections of distinct, shaped parameter arrays connected in a computation graph.</p>"},{"location":"Markdown/mermaidjs-complete-guide/","title":"MermaidJS Complete Diagram Guide","text":"<p>Mermaid is a JavaScript-based diagramming and charting tool that uses text definitions to create diagrams dynamically. This guide covers every diagram type and element available in MermaidJS.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Flowchart</li> <li>Sequence Diagram</li> <li>Class Diagram</li> <li>State Diagram</li> <li>Entity Relationship Diagram</li> <li>User Journey</li> <li>Gantt Chart</li> <li>Pie Chart</li> <li>Quadrant Chart</li> <li>Requirement Diagram</li> <li>Gitgraph Diagram</li> <li>Mindmap</li> <li>Timeline</li> <li>Sankey Diagram</li> <li>XY Chart</li> <li>Block Diagram</li> <li>Packet Diagram</li> <li>Architecture Diagram</li> <li>Kanban Board</li> </ol>"},{"location":"Markdown/mermaidjs-complete-guide/#1-flowchart","title":"1. Flowchart","text":"<p>Flowcharts are composed of nodes and edges. They support various shapes, link styles, and directions.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#directions","title":"Directions","text":"<ul> <li><code>TB</code> / <code>TD</code> - Top to Bottom</li> <li><code>BT</code> - Bottom to Top</li> <li><code>LR</code> - Left to Right</li> <li><code>RL</code> - Right to Left</li> </ul>"},{"location":"Markdown/mermaidjs-complete-guide/#node-shapes","title":"Node Shapes","text":"<pre><code>flowchart LR\n    A[Rectangle] --&gt; B(Rounded Rectangle)\n    B --&gt; C([Stadium])\n    C --&gt; D[[Subroutine]]\n    D --&gt; E[(Database)]\n    E --&gt; F((Circle))</code></pre> <pre><code>flowchart LR\n    G&gt;Asymmetric] --&gt; H{Diamond}\n    H --&gt; I{{Hexagon}}\n    I --&gt; J[/Parallelogram/]\n    J --&gt; K[\\Parallelogram Alt\\]\n    K --&gt; L[/Trapezoid\\]</code></pre> <pre><code>flowchart LR\n    M[\\Trapezoid Alt/] --&gt; N(((Double Circle)))</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#linkedge-types","title":"Link/Edge Types","text":"<pre><code>flowchart LR\n    A --&gt; B\n    A --- C\n    A -.- D\n    A -.-&gt; E\n    A ==&gt; F\n    A ~~~ G\n\n    H -- text --&gt; I\n    J -. text .-&gt; K\n    L == text ==&gt; M</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#link-length","title":"Link Length","text":"<pre><code>flowchart TD\n    A ---&gt; B\n    A ----&gt; C\n    A -----&gt; D</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#subgraphs","title":"Subgraphs","text":"<pre><code>flowchart TB\n    subgraph one [First Subgraph]\n        A1 --&gt; A2\n    end\n    subgraph two [Second Subgraph]\n        B1 --&gt; B2\n    end\n    subgraph three [Third Subgraph]\n        C1 --&gt; C2\n    end\n    one --&gt; two\n    three --&gt; two\n    two --&gt; C2</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#styling","title":"Styling","text":"<pre><code>flowchart LR\n    A:::someclass --&gt; B\n    classDef someclass fill:#f96,stroke:#333,stroke-width:2px\n\n    C --&gt; D\n    style D fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#complete-flowchart-example","title":"Complete Flowchart Example","text":"<pre><code>flowchart TD\n    Start([Start]) --&gt; Input[/Get User Input/]\n    Input --&gt; Validate{Is Valid?}\n    Validate --&gt;|Yes| Process[Process Data]\n    Validate --&gt;|No| Error[Show Error]\n    Error --&gt; Input\n    Process --&gt; Database[(Save to DB)]\n    Database --&gt; Output[\\Display Results\\]\n    Output --&gt; End([End])\n\n    style Start fill:#9f9,stroke:#333\n    style End fill:#f99,stroke:#333</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#2-sequence-diagram","title":"2. Sequence Diagram","text":"<p>Sequence diagrams show how processes interact with each other and in what order.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#participants-and-actors","title":"Participants and Actors","text":"<pre><code>sequenceDiagram\n    participant A as Alice\n    actor B as Bob\n    participant C as Charlie\n\n    A-&gt;&gt;B: Hello Bob!\n    B-&gt;&gt;C: Hi Charlie!\n    C-&gt;&gt;A: Hey Alice!</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#message-types","title":"Message Types","text":"<pre><code>sequenceDiagram\n    participant A\n    participant B\n\n    A-&gt;B: Solid line without arrow\n    A--&gt;B: Dotted line without arrow\n    A-&gt;&gt;B: Solid line with arrowhead\n    A--&gt;&gt;B: Dotted line with arrowhead\n    A-xB: Solid line with cross\n    A--xB: Dotted line with cross\n    A-)B: Solid line with open arrow (async)\n    A--)B: Dotted line with open arrow (async)</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#activations","title":"Activations","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    Client-&gt;&gt;+Server: Request\n    Server-&gt;&gt;+Server: Process\n    Server--&gt;&gt;-Server: Done\n    Server--&gt;&gt;-Client: Response</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#notes","title":"Notes","text":"<pre><code>sequenceDiagram\n    participant A\n    participant B\n\n    Note left of A: Note on left\n    Note right of B: Note on right\n    Note over A: Note over A\n    Note over A,B: Note spanning both\n\n    A-&gt;&gt;B: Message</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#loops-alt-opt-par-critical-break","title":"Loops, Alt, Opt, Par, Critical, Break","text":"<pre><code>sequenceDiagram\n    participant User\n    participant System\n\n    User-&gt;&gt;System: Login Request\n\n    alt Valid Credentials\n        System-&gt;&gt;User: Login Success\n    else Invalid Credentials\n        System-&gt;&gt;User: Login Failed\n    end\n\n    opt Remember Me\n        System-&gt;&gt;System: Store Session\n    end\n\n    loop Health Check\n        System-&gt;&gt;System: Check Status\n    end\n\n    par Parallel Task A\n        System-&gt;&gt;System: Task A\n    and Parallel Task B\n        System-&gt;&gt;System: Task B\n    end\n\n    critical Establish Connection\n        System-&gt;&gt;System: Connect\n    option Connection Timeout\n        System-&gt;&gt;User: Retry\n    end\n\n    break Something Went Wrong\n        System-&gt;&gt;User: Error Response\n    end</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#boxes-grouping","title":"Boxes (Grouping)","text":"<pre><code>sequenceDiagram\n    box Purple Client Side\n        participant Browser\n        participant App\n    end\n    box Green Server Side\n        participant API\n        participant Database\n    end\n\n    Browser-&gt;&gt;App: User Action\n    App-&gt;&gt;API: API Request\n    API-&gt;&gt;Database: Query\n    Database--&gt;&gt;API: Results\n    API--&gt;&gt;App: Response\n    App--&gt;&gt;Browser: Update UI</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#autonumbering","title":"Autonumbering","text":"<pre><code>sequenceDiagram\n    autonumber\n    Alice-&gt;&gt;John: Hello John\n    John--&gt;&gt;Alice: Hi Alice\n    Alice-&gt;&gt;John: How are you?\n    John--&gt;&gt;Alice: I'm good!</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#3-class-diagram","title":"3. Class Diagram","text":"<p>Class diagrams are used in object-oriented modeling to describe the structure of a system.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#basic-class-definition","title":"Basic Class Definition","text":"<pre><code>classDiagram\n    class Animal {\n        +String name\n        +int age\n        +makeSound() void\n        +move() void\n    }</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#visibility-modifiers","title":"Visibility Modifiers","text":"<pre><code>classDiagram\n    class Example {\n        +publicAttribute\n        -privateAttribute\n        #protectedAttribute\n        ~packageAttribute\n        +publicMethod()\n        -privateMethod()\n        #protectedMethod()\n        ~packageMethod()\n    }</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#relationships","title":"Relationships","text":"<pre><code>classDiagram\n    classA &lt;|-- classB : Inheritance\n    classC *-- classD : Composition\n    classE o-- classF : Aggregation\n    classG &lt;-- classH : Association\n    classI -- classJ : Link (Solid)\n    classK &lt;.. classL : Dependency\n    classM &lt;|.. classN : Realization\n    classO .. classP : Link (Dashed)</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#cardinality","title":"Cardinality","text":"<pre><code>classDiagram\n    Customer \"1\" --&gt; \"*\" Order : places\n    Order \"1\" --&gt; \"1..*\" LineItem : contains\n    Product \"1\" --&gt; \"0..*\" LineItem : appears in</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#class-annotations","title":"Class Annotations","text":"<pre><code>classDiagram\n    class Shape {\n        &lt;&lt;interface&gt;&gt;\n        +draw()\n        +area() double\n    }\n\n    class AbstractShape {\n        &lt;&lt;abstract&gt;&gt;\n        #color: String\n        +getColor() String\n    }\n\n    class ShapeService {\n        &lt;&lt;service&gt;&gt;\n        +createShape() Shape\n    }\n\n    class ShapeType {\n        &lt;&lt;enumeration&gt;&gt;\n        CIRCLE\n        SQUARE\n        TRIANGLE\n    }</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#complete-example","title":"Complete Example","text":"<pre><code>classDiagram\n    class Animal {\n        &lt;&lt;abstract&gt;&gt;\n        +String name\n        +int age\n        +makeSound()* void\n        +move() void\n    }\n\n    class Dog {\n        +String breed\n        +makeSound() void\n        +fetch() void\n    }\n\n    class Cat {\n        +boolean indoor\n        +makeSound() void\n        +scratch() void\n    }\n\n    class Pet {\n        &lt;&lt;interface&gt;&gt;\n        +play() void\n        +feed() void\n    }\n\n    Animal &lt;|-- Dog\n    Animal &lt;|-- Cat\n    Pet &lt;|.. Dog\n    Pet &lt;|.. Cat</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#namespaces","title":"Namespaces","text":"<pre><code>classDiagram\n    namespace Animals {\n        class Dog\n        class Cat\n    }\n    namespace Vehicles {\n        class Car\n        class Bike\n    }</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#4-state-diagram","title":"4. State Diagram","text":"<p>State diagrams describe the behavior of a system, showing states and transitions.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#basic-states","title":"Basic States","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Idle\n    Idle --&gt; Processing : Start\n    Processing --&gt; Completed : Finish\n    Completed --&gt; [*]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#composite-states","title":"Composite States","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Active\n\n    state Active {\n        [*] --&gt; Idle\n        Idle --&gt; Running : start\n        Running --&gt; Idle : stop\n        Running --&gt; Running : process\n    }\n\n    Active --&gt; Inactive : deactivate\n    Inactive --&gt; Active : activate\n    Inactive --&gt; [*]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#fork-and-join","title":"Fork and Join","text":"<pre><code>stateDiagram-v2\n    state fork_state &lt;&lt;fork&gt;&gt;\n    state join_state &lt;&lt;join&gt;&gt;\n\n    [*] --&gt; fork_state\n    fork_state --&gt; State1\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    State1 --&gt; join_state\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n\n    join_state --&gt; FinalState\n    FinalState --&gt; [*]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#choice","title":"Choice","text":"<pre><code>stateDiagram-v2\n    state check_result &lt;&lt;choice&gt;&gt;\n\n    [*] --&gt; Processing\n    Processing --&gt; check_result\n    check_result --&gt; Success : if valid\n    check_result --&gt; Failure : if invalid\n    Success --&gt; [*]\n    Failure --&gt; [*]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#notes_1","title":"Notes","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Active\n    Active --&gt; Inactive\n\n    note right of Active\n        This is an active state\n        It can process events\n    end note\n\n    note left of Inactive : This is inactive</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#concurrency","title":"Concurrency","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Active\n\n    state Active {\n        [*] --&gt; ProcessA\n        --\n        [*] --&gt; ProcessB\n        --\n        [*] --&gt; ProcessC\n    }\n\n    Active --&gt; [*]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#direction","title":"Direction","text":"<pre><code>stateDiagram-v2\n    direction LR\n    [*] --&gt; A\n    A --&gt; B\n    B --&gt; C\n    C --&gt; [*]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#5-entity-relationship-diagram","title":"5. Entity Relationship Diagram","text":"<p>ER diagrams show relationships between entities in a database.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#relationship-types","title":"Relationship Types","text":"<pre><code>erDiagram\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--|{ LINE-ITEM : contains\n    CUSTOMER }|..|{ DELIVERY-ADDRESS : uses\n    PRODUCT ||--o{ LINE-ITEM : \"ordered in\"</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#cardinality-notation","title":"Cardinality Notation","text":"Value Meaning <code>\\|o</code> Zero or one <code>\\|\\|</code> Exactly one <code>}o</code> Zero or more <code>}\\|</code> One or more"},{"location":"Markdown/mermaidjs-complete-guide/#entity-attributes","title":"Entity Attributes","text":"<pre><code>erDiagram\n    CUSTOMER {\n        int id PK\n        string name\n        string email UK\n        date created_at\n    }\n\n    ORDER {\n        int id PK\n        int customer_id FK\n        date order_date\n        decimal total\n    }\n\n    PRODUCT {\n        int id PK\n        string name\n        decimal price\n        string description\n    }\n\n    LINE_ITEM {\n        int id PK\n        int order_id FK\n        int product_id FK\n        int quantity\n    }\n\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--|{ LINE_ITEM : contains\n    PRODUCT ||--o{ LINE_ITEM : included_in</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#attribute-types","title":"Attribute Types","text":"<pre><code>erDiagram\n    ENTITY {\n        type attribute_name PK \"Primary Key\"\n        type attribute_name FK \"Foreign Key\"\n        type attribute_name UK \"Unique Key\"\n    }</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#6-user-journey","title":"6. User Journey","text":"<p>User journey diagrams describe the steps a user takes to complete a task.</p> <pre><code>journey\n    title My Working Day\n    section Morning\n        Wake up: 1: Me\n        Breakfast: 3: Me\n        Commute: 2: Me, Bus\n    section Work\n        Check emails: 3: Me\n        Meeting: 2: Me, Colleagues\n        Lunch: 5: Me\n        Coding: 4: Me\n    section Evening\n        Commute home: 2: Me, Bus\n        Dinner: 5: Me, Family\n        Sleep: 5: Me</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#score-values","title":"Score Values","text":"<p>Scores range from 1 to 5: - 1: Very negative experience - 2: Negative experience - 3: Neutral experience - 4: Positive experience - 5: Very positive experience</p>"},{"location":"Markdown/mermaidjs-complete-guide/#7-gantt-chart","title":"7. Gantt Chart","text":"<p>Gantt charts illustrate project schedules.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#basic-gantt","title":"Basic Gantt","text":"<pre><code>gantt\n    title Project Timeline\n    dateFormat YYYY-MM-DD\n\n    section Planning\n        Requirements    :a1, 2024-01-01, 7d\n        Design          :a2, after a1, 5d\n\n    section Development\n        Backend         :b1, after a2, 14d\n        Frontend        :b2, after a2, 14d\n        Integration     :b3, after b1, 5d\n\n    section Testing\n        Unit Tests      :c1, after b3, 7d\n        QA Testing      :c2, after c1, 7d</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#task-types","title":"Task Types","text":"<pre><code>gantt\n    title Task Types Example\n    dateFormat YYYY-MM-DD\n\n    section Tasks\n        Normal task         :a1, 2024-01-01, 3d\n        Critical task       :crit, a2, after a1, 2d\n        Done task           :done, a3, after a2, 2d\n        Active task         :active, a4, after a3, 3d\n        Milestone           :milestone, m1, after a4, 0d</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#sections-and-dependencies","title":"Sections and Dependencies","text":"<pre><code>gantt\n    title Software Development Lifecycle\n    dateFormat YYYY-MM-DD\n    excludes weekends\n\n    section Analysis\n        Gather Requirements     :done, req, 2024-01-01, 5d\n        Analyze Requirements    :done, ana, after req, 3d\n\n    section Design\n        System Design           :active, des, after ana, 5d\n        UI/UX Design            :uix, after ana, 7d\n\n    section Implementation\n        Core Development        :dev, after des, 15d\n        API Development         :api, after des, 10d\n\n    section Testing\n        Testing                 :test, after dev, 10d\n\n    section Deployment\n        Deployment              :milestone, dep, after test, 0d</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#8-pie-chart","title":"8. Pie Chart","text":"<pre><code>pie showData\n    title Browser Market Share\n    \"Chrome\" : 65\n    \"Safari\" : 19\n    \"Firefox\" : 4\n    \"Edge\" : 4\n    \"Other\" : 8</code></pre> <pre><code>pie\n    title Project Budget Allocation\n    \"Development\" : 45\n    \"Marketing\" : 20\n    \"Operations\" : 15\n    \"Support\" : 12\n    \"R&amp;D\" : 8</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#9-quadrant-chart","title":"9. Quadrant Chart","text":"<p>Quadrant charts plot items on a 2x2 grid for prioritization or classification.</p> <pre><code>quadrantChart\n    title Product Feature Prioritization\n    x-axis Low Effort --&gt; High Effort\n    y-axis Low Impact --&gt; High Impact\n    quadrant-1 Plan\n    quadrant-2 Do First\n    quadrant-3 Delegate\n    quadrant-4 Eliminate\n\n    Feature A: [0.8, 0.9]\n    Feature B: [0.3, 0.8]\n    Feature C: [0.7, 0.3]\n    Feature D: [0.2, 0.2]\n    Feature E: [0.5, 0.5]\n    Feature F: [0.9, 0.7]</code></pre> <pre><code>quadrantChart\n    title Skill Assessment\n    x-axis Low Competence --&gt; High Competence\n    y-axis Low Interest --&gt; High Interest\n    quadrant-1 Develop\n    quadrant-2 Leverage\n    quadrant-3 Ignore\n    quadrant-4 Delegate\n\n    Python: [0.9, 0.8]\n    JavaScript: [0.7, 0.7]\n    Rust: [0.3, 0.9]\n    Java: [0.6, 0.3]\n    Go: [0.4, 0.6]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#10-requirement-diagram","title":"10. Requirement Diagram","text":"<p>Requirement diagrams show requirements and their relationships.</p> <pre><code>requirementDiagram\n    requirement user_auth {\n        id: REQ-001\n        text: The system shall authenticate users\n        risk: high\n        verifymethod: test\n    }\n\n    requirement password_policy {\n        id: REQ-002\n        text: Passwords must be at least 8 characters\n        risk: medium\n        verifymethod: inspection\n    }\n\n    requirement session_mgmt {\n        id: REQ-003\n        text: Sessions shall expire after 30 minutes\n        risk: low\n        verifymethod: demonstration\n    }\n\n    element auth_module {\n        type: module\n        docRef: auth_design_doc\n    }\n\n    element login_test {\n        type: testCase\n        docRef: test_plan_001\n    }\n\n    user_auth - derives -&gt; password_policy\n    user_auth - derives -&gt; session_mgmt\n    auth_module - satisfies -&gt; user_auth\n    login_test - verifies -&gt; user_auth</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#relationship-types_1","title":"Relationship Types","text":"<ul> <li><code>contains</code> - Requirement contains another requirement</li> <li><code>copies</code> - Requirement is a copy of another</li> <li><code>derives</code> - Requirement derives from another</li> <li><code>satisfies</code> - Element satisfies requirement</li> <li><code>verifies</code> - Element verifies requirement</li> <li><code>refines</code> - Requirement refines another</li> <li><code>traces</code> - Requirement traces to another</li> </ul>"},{"location":"Markdown/mermaidjs-complete-guide/#11-gitgraph-diagram","title":"11. Gitgraph Diagram","text":"<p>Gitgraph diagrams visualize git branching and merging.</p> <pre><code>gitGraph\n    commit id: \"Initial\"\n    commit id: \"Add README\"\n    branch develop\n    checkout develop\n    commit id: \"Setup project\"\n    commit id: \"Add core features\"\n    branch feature/auth\n    checkout feature/auth\n    commit id: \"Add login\"\n    commit id: \"Add logout\"\n    checkout develop\n    merge feature/auth\n    branch feature/dashboard\n    checkout feature/dashboard\n    commit id: \"Create dashboard\"\n    commit id: \"Add widgets\"\n    checkout develop\n    merge feature/dashboard\n    checkout main\n    merge develop tag: \"v1.0.0\"\n    commit id: \"Hotfix\"</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#customization-options","title":"Customization Options","text":"<pre><code>%%{init: { 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'production'}} }%%\ngitGraph\n    commit id: \"1\"\n    commit id: \"2\"\n    branch staging\n    checkout staging\n    commit id: \"3\"\n    checkout production\n    merge staging\n    commit id: \"4\"</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#cherry-pick","title":"Cherry-pick","text":"<pre><code>gitGraph\n    commit id: \"A\"\n    commit id: \"B\"\n    branch develop\n    commit id: \"C\"\n    commit id: \"D\"\n    checkout main\n    cherry-pick id: \"C\"\n    commit id: \"E\"</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#12-mindmap","title":"12. Mindmap","text":"<p>Mindmaps visualize hierarchical information.</p> <pre><code>mindmap\n    root((Project Planning))\n        Research\n            Market Analysis\n            Competitor Review\n            User Surveys\n        Design\n            Wireframes\n            Mockups\n            Prototypes\n        Development\n            Frontend\n                React\n                CSS\n                TypeScript\n            Backend\n                Node.js\n                Database\n                API\n        Testing\n            Unit Tests\n            Integration Tests\n            E2E Tests\n        Deployment\n            CI/CD\n            Monitoring\n            Scaling</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#node-shapes_1","title":"Node Shapes","text":"<pre><code>mindmap\n    root)Cloud Shape(\n        (Rounded Square)\n            [Square]\n                {{Hexagon}}\n        ))Bang((\n            Default Text</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#icons-and-classes","title":"Icons and Classes","text":"<pre><code>mindmap\n    root((Central Topic))\n        ::icon(fa fa-book)\n        Topic A\n            Subtopic A1\n            Subtopic A2\n        Topic B\n            ::icon(fa fa-code)\n            Subtopic B1\n        Topic C</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#13-timeline","title":"13. Timeline","text":"<p>Timeline diagrams show events over time.</p> <pre><code>timeline\n    title History of Web Development\n\n    section 1990s\n        1991 : First website created\n        1993 : HTML 1.0\n        1995 : JavaScript created\n             : PHP released\n        1996 : CSS 1.0\n        1998 : Google founded\n\n    section 2000s\n        2004 : Facebook launched\n        2005 : YouTube launched\n             : AJAX popularized\n        2006 : jQuery released\n        2008 : HTML5 draft\n\n    section 2010s\n        2010 : AngularJS\n        2013 : React released\n        2014 : Vue.js released\n        2015 : ES6/ES2015\n\n    section 2020s\n        2020 : Remote work boom\n        2022 : ChatGPT launched\n        2023 : AI integration</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#14-sankey-diagram","title":"14. Sankey Diagram","text":"<p>Sankey diagrams visualize flow quantities between nodes.</p> <pre><code>sankey-beta\n\n%% source, target, value\nElectricity,Residential,30\nElectricity,Commercial,25\nElectricity,Industrial,45\n\nNatural Gas,Residential,20\nNatural Gas,Commercial,15\nNatural Gas,Industrial,35\n\nCoal,Industrial,50\nCoal,Power Plants,100\n\nPower Plants,Electricity,100\n\nRenewable,Electricity,40</code></pre> <pre><code>sankey-beta\n\nBudget,Marketing,250\nBudget,Development,400\nBudget,Operations,200\nBudget,HR,150\n\nMarketing,Digital,150\nMarketing,Traditional,100\n\nDevelopment,Frontend,150\nDevelopment,Backend,150\nDevelopment,DevOps,100\n\nOperations,Infrastructure,100\nOperations,Support,100</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#15-xy-chart","title":"15. XY Chart","text":"<p>XY charts display data on a coordinate system.</p>"},{"location":"Markdown/mermaidjs-complete-guide/#bar-chart","title":"Bar Chart","text":"<pre><code>xychart-beta\n    title \"Monthly Sales\"\n    x-axis [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec]\n    y-axis \"Revenue (in $1000)\" 0 --&gt; 150\n    bar [50, 60, 75, 90, 80, 95, 110, 120, 100, 85, 95, 130]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#line-chart","title":"Line Chart","text":"<pre><code>xychart-beta\n    title \"Temperature Trend\"\n    x-axis [Mon, Tue, Wed, Thu, Fri, Sat, Sun]\n    y-axis \"Temperature (\u00b0C)\" 0 --&gt; 40\n    line [22, 24, 26, 28, 25, 23, 21]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#combined-bar-and-line","title":"Combined Bar and Line","text":"<pre><code>xychart-beta\n    title \"Sales vs Target\"\n    x-axis [Q1, Q2, Q3, Q4]\n    y-axis \"Amount ($)\" 0 --&gt; 200\n    bar [120, 150, 140, 180]\n    line [100, 130, 160, 190]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#16-block-diagram","title":"16. Block Diagram","text":"<p>Block diagrams show system components and their relationships.</p> <pre><code>block-beta\n    columns 3\n\n    Frontend:3\n\n    space API space\n\n    block:backend:3\n        columns 3\n        Auth Database Cache\n    end\n\n    space Storage space</code></pre> <pre><code>block-beta\n    columns 5\n\n    User space:3 Admin\n\n    space down1&lt;[\"  \"]&gt;(down) space down2&lt;[\"  \"]&gt;(down) space\n\n    space WebApp:3 space\n\n    space down3&lt;[\"  \"]&gt;(down) space\n\n    space API:3 space\n\n    space down4&lt;[\"  \"]&gt;(down) space\n\n    DB[(\"Database\")]:5</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#block-shapes","title":"Block Shapes","text":"<pre><code>block-beta\n    A[\"Rectangle\"]\n    B(\"Rounded\")\n    C((\"Circle\"))\n    D{{\"Hexagon\"}}\n    E[/\"Parallelogram\"/]\n    F[\\\\\"Parallelogram Alt\"\\\\]\n    G[(\"Cylinder\")]\n    H&gt;\"Asymmetric\"]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#17-packet-diagram","title":"17. Packet Diagram","text":"<p>Packet diagrams show the structure of network packets.</p> <pre><code>packet-beta\n    0-15: \"Source Port\"\n    16-31: \"Destination Port\"\n    32-63: \"Sequence Number\"\n    64-95: \"Acknowledgment Number\"\n    96-99: \"Data Offset\"\n    100-102: \"Reserved\"\n    103: \"NS\"\n    104: \"CWR\"\n    105: \"ECE\"\n    106: \"URG\"\n    107: \"ACK\"\n    108: \"PSH\"\n    109: \"RST\"\n    110: \"SYN\"\n    111: \"FIN\"\n    112-127: \"Window Size\"\n    128-143: \"Checksum\"\n    144-159: \"Urgent Pointer\"\n    160-191: \"Options (if Data Offset &gt; 5)\"</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#18-architecture-diagram","title":"18. Architecture Diagram","text":"<p>Architecture diagrams show system architecture with cloud/infrastructure components.</p> <pre><code>architecture-beta\n    group cloud(cloud)[Cloud Platform]\n\n    service users(internet)[Users] in cloud\n    service lb(server)[Load Balancer] in cloud\n    service api(server)[API Server] in cloud\n    service db(database)[Database] in cloud\n\n    users:R --&gt; L:lb\n    lb:R --&gt; L:api\n    api:R --&gt; L:db</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#19-kanban-board","title":"19. Kanban Board","text":"<p>Kanban diagrams visualize work items across workflow stages.</p> <pre><code>kanban\n    column1[Todo]\n        task1[Design homepage]\n        task2[Write documentation]\n        task3[Create database schema]\n\n    column2[In Progress]\n        task4[Implement auth]\n        task5[Build API endpoints]\n\n    column3[Review]\n        task6[Code review: login]\n\n    column4[Done]\n        task7[Setup project]\n        task8[Configure CI/CD]</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#theming-and-configuration","title":"Theming and Configuration","text":""},{"location":"Markdown/mermaidjs-complete-guide/#built-in-themes","title":"Built-in Themes","text":"<p>Mermaid supports several built-in themes: - <code>default</code> - <code>neutral</code> - <code>dark</code> - <code>forest</code> - <code>base</code></p> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart LR\n    A --&gt; B --&gt; C --&gt; D</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#custom-theme-variables","title":"Custom Theme Variables","text":"<pre><code>%%{init: {\n    'theme': 'base',\n    'themeVariables': {\n        'primaryColor': '#ff6b6b',\n        'primaryTextColor': '#fff',\n        'primaryBorderColor': '#c92a2a',\n        'lineColor': '#868e96',\n        'secondaryColor': '#ffd43b',\n        'tertiaryColor': '#e9ecef'\n    }\n}}%%\nflowchart LR\n    A[Start] --&gt; B{Decision}\n    B --&gt;|Yes| C[Action 1]\n    B --&gt;|No| D[Action 2]\n    C --&gt; E[End]\n    D --&gt; E</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#directives-and-comments","title":"Directives and Comments","text":""},{"location":"Markdown/mermaidjs-complete-guide/#comments","title":"Comments","text":"<pre><code>flowchart LR\n    %% This is a comment\n    A --&gt; B\n    %% Another comment\n    B --&gt; C</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#directives","title":"Directives","text":"<pre><code>%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart LR\n    A --&gt; B</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#accessibility","title":"Accessibility","text":"<p>Mermaid supports accessibility descriptions:</p> <pre><code>flowchart LR\n    accTitle: Simple Flow\n    accDescr: A flow showing three connected nodes A, B, and C\n\n    A --&gt; B --&gt; C</code></pre>"},{"location":"Markdown/mermaidjs-complete-guide/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Keep diagrams simple: Complex diagrams become hard to read</li> <li>Use meaningful labels: Clear labels improve understanding</li> <li>Choose the right diagram type: Match the diagram to your data</li> <li>Use subgraphs for organization: Group related elements</li> <li>Apply consistent styling: Use themes or custom styles consistently</li> <li>Add titles and descriptions: Improve accessibility and context</li> <li>Test rendering: Different renderers may have slight variations</li> </ol>"},{"location":"Markdown/mermaidjs-complete-guide/#resources","title":"Resources","text":"<ul> <li>Mermaid Official Documentation</li> <li>Mermaid Live Editor</li> <li>GitHub Repository</li> </ul> <p>This guide covers MermaidJS version 10.x. Some features may vary in different versions.</p>"}]}